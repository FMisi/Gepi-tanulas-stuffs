# -*- coding: utf-8 -*-
"""ml_10_gyakorlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M-Z-YWsg7FTNNrpxSVJS9wJ5K3ln2CAR

# Gyakorló feladatok

## 1. feladat
Az alábbi arcérzelem képi adatbázisból vegyük az első 50 képet. Vizualizáljuk ezeket a képeket 2D-ben, úgy, hogy a hasonló képek közelebb vannak egymáshoz!
"""

# arcérzelem képi adatbázis
import pandas as pd
data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/images/fer2013_1K.csv')
data

#48x48 szürke képek egy stringbe vannak kódolva, így lesz belőle tömb
import numpy as np
def str2img(img):
  return np.reshape(np.asarray(img.split(' '), np.uint8), (48,48))

# képeket most csak a pixelekkel írjuk le
features = []
for x in data.pixels.tolist():
  features.append(str2img(x).flatten())

!wget https://raw.githubusercontent.com/christiansafka/img2vec/master/img2vec_pytorch/img_to_vec.py

from img_to_vec import Img2Vec
img2vec = Img2Vec()

len(features[0])

from PIL import Image

imgvecs = []
for i in data.pixels:
  imgvec = img2vec.get_vec(Image.fromarray(str2img(i)).convert('RGB'))
  imgvecs.append(imgvec)

# Az SVD 2 dimenzióba képezi le a képeket
from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=2) # az első 2 komponenst tartjuk csak meg (2Dbe mappelünk)
p2d = svd.fit_transform(imgvecs)
p2d # minden képhez egy 2D vektor

import matplotlib.pyplot as plt
plt.figure(figsize=(15,15))
plt.scatter(p2d[:50,0], p2d[:50,1], c=data[:50].emotion) # az első 50 kép SVD koordinátáira kirakunk egy pöttyöt, aminek színe (c=) a kép címkéje

### bonus feladat: magukat a képeket rajzoljuk ki a pötty helyére
### segített: https://stackoverflow.com/questions/4860417/placing-custom-images-in-a-plot-window-as-custom-data-markers-or-to-annotate-t
from matplotlib.offsetbox import AnnotationBbox, OffsetImage

plt.figure(figsize=(15,15))
ax = plt.subplot(111)

# kép x és y tengelyét állítsuk az SVD min-max értékeire
ax.set(xlim=(min(p2d[:50,0]), max(p2d[:50,0])), ylim=(min(p2d[:50,1]), max(p2d[:50,1])))

for i in range(50):
  img = str2img(data.pixels[i])
  xy = p2d[i]
  ax.add_artist(AnnotationBbox(OffsetImage(img), xy))

"""## 2. feladat

A arcérzelem adatbázist klaszterezzük és minden klaszterből néhány egyedet jelenítsünk meg a 2D térben!
"""

### K-means kluszterezése az ezer képnek a pixelvektorok alapján (az eredeti térben)
from sklearn.cluster import KMeans
k=5
kmeans = KMeans(n_clusters=k).fit(imgvecs)

### Összegyűjtjük egy szótárba az egyes klaszterekbe sorolt képek indexeit
cluster = {}
for j in range(k):
  cluster[j] = [i for i, e in enumerate(kmeans.labels_) if e == j] # j. kluszterbe sorolt képek indexei

import matplotlib.colors as mcolors

plt.figure(figsize=(15,15))
ax = plt.subplot(111)
# kép x és y tengelyét állítsuk az ezer kép SVD min-max értékeire
ax.set(xlim=(min(p2d[:,0]), max(p2d[:,0])), ylim=(min(p2d[:,1]), max(p2d[:,1])))

for j in range(k): # minden klaszter
  for i in range(5): # első 5 képét jelenítjük meg
    img = str2img(data.pixels[ cluster[j][i] ]) # cluster[j][i] az index az eredeti adatbázisban
    xy = p2d[ cluster[j][i] ] # 2Dre mappelt koordináták
    ax.add_artist(AnnotationBbox(OffsetImage(img), xy, bboxprops = dict(edgecolor=list(mcolors.BASE_COLORS)[j]))) # a kereteket az egyes klaszterek színére állítjuk# -*- coding: utf-8 -*-
"""ml_1_gyakorlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ypCE-iah7WUTK_LteCulvWbbe25_BXuY

# Gyakorló feladatok megoldása

Egy string 3 leggyakoribb szavának kilistázás
"""

s = 'a a alma körte b b alma alma c d e f g haha haha'
f={}
for w in s.split():
  if w in f:
    f[w] += 1
  else:
    f[w] = 1
# az f szótárat érték szerint (key=f.get) rendezzük csökkenő sorrendbe (reversre=True)
# majd kérem az eredmény lista első három elemét
sorted(f, key=f.get, reverse=True)[:3]

"""Másik megoldás:"""

from collections import Counter #pythonban mindenre van külső csomag :)

x = Counter(s.split())
print(sorted(x, key=x.get, reverse=True)[:5])# -*- coding: utf-8 -*-
"""ml_2_gyakorlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W-BxP89xxeceL3q2bAPndPfZOr6ZCaOV

# Gyakorló feladatok
1. Dohányzik a két legidősebb ember?
"""

# adattábla betöltése
import pandas as pd
df = pd.read_csv("https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/MASS/survey.csv")

# 3 legidősebb ember életkora
sorted(df.Age)[-3:] # az Age oszlopot sorba rendezzük (eredménye lista), és kérem a 3 utolsó elemet

# a két legidősebb ember 70 év feletti, kérem az ő dohányzási változójukat
df.loc[df.Age>70,'Smoke']

"""Ez a megoldási mód jól példázza az adatelemzés és szoftverfejlesztés közti különbséget. Két egyszerű, egymásra épülő lekérdezéssel meg tudtuk válaszolni a konkrét kérdést, nem fejlesztettünk általános szoftvert...

2. Mi az átlagos Wr.Hnd a férfiaknál?
"""

# Először leszűrjük a férfiak feltételes résztömbjét, majd kérjük annak Wr.Hnd oszlopát. Ennek a Series-nek számoljuk az átlagát.
df[df.Sex=='Male']['Wr.Hnd'].mean()# -*- coding: utf-8 -*-
"""ml_4_gyakorlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rGWxE88NqFlRF6ZpDXSf3U8w7KqtqJEG

# Gyakorló fealdatok

1. Hajts végre egy osztályozási feladatot a [survey adatbázison](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/survey.html)
"""

import pandas as pd
df = pd.read_csv("https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/MASS/survey.csv")
df.head()

df.shape

"""ahol azt akarjuk predikálni, hogy kézkulcsolásnál melyik kéz van felül (`Fold`), az összes többi oszlop, mint jellemző alapján!"""

# Osztálycímke:
classlabel = df.Fold
# Jellemzőtér:
features = df.drop('Fold',axis=1) # a drop() eltávolít egy oszlopot (ha axis=1), létrehoz egy új dataframe-et

### Diszkrét jellemzők átkonvertálása
from sklearn import preprocessing
ohe = preprocessing.OneHotEncoder() # one hot encoding
ohe_features = ohe.fit_transform(features)

# 'Input contains NaN', valamit kezdenünk kell a NaNokkal
df.dropna().shape

# 168 példánk marad ha töröljük a NaNt tartalmazó sorokat
classlabel = df.dropna().Clap
features = df.dropna().drop('Clap',axis=1)

# így már lefut a one hot encoder
ohe_features = ohe.fit_transform(features)

### döntési fa osztályozót használunk
from sklearn import tree
dt = tree.DecisionTreeClassifier()

### Tanító- és kiértékelő adatbázisra bontás
# legyen a tanító adatbázis az első 120 elem, a többi a kiértékelő adatbázis
train_features = ohe_features[:120]
train_labels = classlabel[:120]
test_features = ohe_features[120:]
test_labels = classlabel[120:]

dt.fit(train_features, train_labels) # tanítás a tanító adatbázison
prediction = dt.predict(test_features) # predikció a kiértékelő adatbázison

### Kiértékelés találati aránnyal (accuracy)
from sklearn.metrics import accuracy_score
accuracy_score(prediction, test_labels)

### Mindig hasonlítsuk eredményeinket baseline szabályhoz!
from sklearn.dummy import DummyClassifier
dummy_clf = DummyClassifier(strategy="most_frequent")
dummy_clf.fit(train_features, train_labels)
baseline_prediction = dummy_clf.predict(test_features)
accuracy_score(baseline_prediction, test_labels)

"""2. Értékeld ki az legfeljebb 1, 2, 3 mélységűre korlátozott döntési fákat is!"""

for d in range(1,4):
  dt = tree.DecisionTreeClassifier() # fa mélységkorlátozással
  dt.fit(train_features, train_labels)
  prediction = dt.predict(test_features)
  print(d,":",accuracy_score(prediction, test_labels))# -*- coding: utf-8 -*-
"""ml_5_deep_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eI8j0CRj2L8fbDbat5Dl35FC0Bg9BU5u

# Deep Learning
Olvasd el az [elméleti bevezetőt](http://inf.u-szeged.hu/~rfarkas/deep_learning.html).

### Futtatás GPU-n

A mély neurális hálók tanítása nagyon számításigényes, viszont visszavezetve mátrixműveletekre nagyon jól párhuzamosítható GPU-n. Érdemes a Google Colab-ban is átváltani GPU-ra. Ezt az Edit>Notebook settings menüben tehetjük meg GPU-t választva hardveres gyorsításra. Ha CPU-ról átvátunk GPU-ra akkor újra kell futtatni a teljes notebookot!

A Cuda egy alacsony szintű szoftverréteg mátrixműveletek GPU-n való nagyon hatékony megvalósítására. E fölé épülnek a deep learning keretrendszerek, pl.  [PyTorch](https://pytorch.org/) és a [Tensorflow](https://www.tensorflow.org/).
"""

### PyTorch deep learning keretrendszert használjuk: https://pytorch.org
import torch

### Futtatási környezet előkészítése

# Cuda inicializálása
torch.backends.cudnn.deterministic = True

# a neurális hálók tanításánál a véletlenszám-generálásnak nagy szerepe van
# érdemes a random seedet fixálni, hogy minden futtatásra ugyanazt az eredményt kapjuk
SEED = 202004
torch.manual_seed(SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

"""# Szövegosztályozás mély tanulással

Az [elöző](https://colab.research.google.com/drive/1Ve2FOeA7ceEgS0eqL-31CUFFT33s_PDm) órán megoldott szövegosztályozási feladatra fogunk adni itt egy mély gépi tanulási megoldást. Ugyanaz a feladat, véleményosztályozás. Ugyanazon az adatbázison, ugyanazon kiértékelési metrikát használjuk, így az eredmények összehasonlíthatóak a klasszikus gépi tanulási eredményekkel.
"""

import pandas as pd
train_data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/train.tsv', sep='\t')
test_data  = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/test.tsv', sep='\t')

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
vectorizer = CountVectorizer()
cv_counts = vectorizer.fit_transform(train_data.text)
idf_transformer = TfidfTransformer(use_idf=True).fit(cv_counts)
features = idf_transformer.transform(cv_counts)
test_features = idf_transformer.transform(vectorizer.transform(test_data.text))

features

from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score
model = SGDClassifier().fit(features, train_data.label)
accuracy_score(y_true=test_data.label, y_pred=model.predict(test_features))

"""## Egyszerű neurális hálózat"""

### ritka mátrixot tensor formátumra alakítjuk
import numpy as np
X_train_tensor = torch.from_numpy(features.todense()).float()
X_test_tensor  = torch.from_numpy(test_features.todense()).float()

train_data.label

### PyTorch-ban még a célváltozó sem lehet diszkrét...
### A LabelEncoder véletlenszerűen Int-eket rendel az egyes értékekhez
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Y_train_tensor = torch.as_tensor(le.fit_transform(train_data.label))
Y_test_tensor  = torch.as_tensor(le.transform(test_data.label))

### Jellemzőtér (=bemeneti réteg) dimenziói és célváltozók száma (=kimeneti réteg dimenziója)
VOCAB_SIZE = len(vectorizer.vocabulary_)
OUT_CLASSES = 3

### Linear Machine, LM
### A legegyszerűbb neurális háló (ami megegyezik a lineáris géppel)
### a kimeneti neuron össze vannak kötve a bementiekkel (mindegyik mindegyikkel)

class LM_Network(torch.nn.Module):
     def __init__(self,vocab_size,out_classes):
        super().__init__()
        self.linear = torch.nn.Linear(vocab_size,out_classes)
     def forward(self,x):
        return self.linear(x)

model = LM_Network(VOCAB_SIZE,OUT_CLASSES)
print(model)

# összesen ennyi paramétert kell tanítanunk:
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

count_parameters(model)

#predikció a random hálóval
model(X_train_tensor[1:3])

### Multi Layer Perceptron, MLP
### 1 rejtett réteget tartalmazó neuárlis hálózat

class MLP_Network(torch.nn.Module):
  def __init__(self,vocab_size,hidden_units,num_classes):
      super().__init__()
      #First fully connected layer
      self.fc1 = torch.nn.Linear(vocab_size,hidden_units)
      #Second fully connected layer
      self.fc2 = torch.nn.Linear(hidden_units,num_classes)
      #Final output of sigmoid function
      self.sigmoid = torch.nn.Sigmoid()

  def forward(self,x):
      y1 = self.sigmoid(self.fc1(x))
      output = self.sigmoid(self.fc2(y1))
      return output

HIDDEN_UNITS = 100
model = MLP_Network(VOCAB_SIZE, HIDDEN_UNITS, OUT_CLASSES)
print(model)
print(count_parameters(model), "tanulandó paraméter")

### Kiértékelő függvény
def accuracy(preds, y):
    max_preds = preds.argmax(dim = 1, keepdim = True) # a 3 osztályra adott kimeneti érték közül melyik a legnagyobb
    correct = max_preds.squeeze(1).eq(y)
    return correct.sum(dtype=float) / y.shape[0]

### ha az epoch végén egy független validációs halmazon is ki akarjuk értékelni a modellt:
def evaluate(model, iterator):
    epoch_acc = 0
    model.eval()  # inicializálás
    with torch.no_grad():
        for batch in iterator:
            # predikció
            predictions = model(batch[0])
            # kiértékelés
            acc = accuracy(predictions, batch[1].long())
            epoch_acc += acc.item()

    return epoch_acc / len(iterator)

Y_test_tensor

from torch.utils.data import Dataset, TensorDataset
train_data = TensorDataset(X_train_tensor, Y_train_tensor)
test_data  = TensorDataset(X_test_tensor,  Y_test_tensor)

### Ha egy adatbázison akarunk végigmenni akkor ahhoz iterátort kell definiálni
from torch.utils.data import DataLoader
train_loader = DataLoader(train_data,batch_size=16, shuffle=True)

# random hálózat kiértékelése az egész adatbázison
evaluate(model, train_loader)

### A tanítás során többször végigmegyünk a tanító adatbázison (egy kör egy epoch)
def train(model, iterator, optimizer, criterion):
    # minden epoch végén ellenőrízni fogjuk az accuracyt
    epoch_acc = 0

    model.train() # inicializálás
    for batch in iterator:
        # predikáljuk le a tanító példákat az aktuális paraméterekkel:
        optimizer.zero_grad()
        predictions = model(batch[0])

        # a háló aktuális paraméterivel ennyi a hiba a batchen:
        loss = criterion(predictions, batch[1].long())
        acc = accuracy(predictions, batch[1].long())

        # hibavisszaterjesztéssel (backpropagation) javítunk a paramétereken:
        loss.backward()
        optimizer.step()

        epoch_acc += acc.item()

    return epoch_acc / len(iterator)

# Commented out IPython magic to ensure Python compatibility.
# ### Neurális hálózat tanítása
# %%time
# NUM_EPOCHS = 10
# BATCH_SIZE = 64
# 
# #Neurális háló architektúra megadása
# model = MLP_Network(VOCAB_SIZE,HIDDEN_UNITS,OUT_CLASSES)
# 
# #optimalizáló eljárás
# import torch.optim as optim
# optimizer = optim.Adam(model.parameters()) # ADAM optimalizáló algoritmus
# 
# #célfüggvény
# import torch.nn as nn
# loss_fun = nn.CrossEntropyLoss()
# 
# iterator = DataLoader(train_data,batch_size=BATCH_SIZE, shuffle=True)
# for i in range(NUM_EPOCHS):
#    print(i, ". epoch acc:", train(model, iterator, optimizer, loss_fun))

### Kiértékelés a teszt halmazon
test_loader = DataLoader(test_data,batch_size=16, shuffle=True)
evaluate(model, test_loader)

# Commented out IPython magic to ensure Python compatibility.
# ### Futtassunk mindent GPU-n!
# ### Mindent át kell pakolni a GPU memóriájába...
# 
# %%time
# NUM_EPOCHS = 10
# BATCH_SIZE = 64
# 
# #Initialize model
# model = MLP_Network(VOCAB_SIZE,HIDDEN_UNITS,OUT_CLASSES).to(device)
# 
# #Initialize optimizer
# import torch.optim as optim
# optimizer = optim.Adam(model.parameters()) # ADAM optimalizáló algoritmus
# import torch.nn as nn
# loss_fun = nn.CrossEntropyLoss().to(device)
# 
# X_train_tensor = X_train_tensor.to(device)
# Y_train_tensor = Y_train_tensor.to(device)
# train_data = TensorDataset(X_train_tensor, Y_train_tensor)
# iterator = DataLoader(train_data,batch_size=BATCH_SIZE, shuffle=True)
# 
# for i in range(NUM_EPOCHS):
#    print(i, ". epoch acc:", train(model, iterator, optimizer, loss_fun))

X_test_tensor = X_test_tensor.to(device)
Y_test_tensor = Y_test_tensor.to(device)
test_data = TensorDataset(X_test_tensor, Y_test_tensor)
test_loader = DataLoader(test_data,batch_size=16, shuffle=True)
evaluate(model, test_loader)

"""## Konvolúciós Neurális Hálózatok (CNN)

Egy ún **Konvulúciós Neurális Hálózatot** fogunk építani és tanítani a szövegosztályozási feladathoz (lásd [olvasólecke](https://www.inf.u-szeged.hu/~rfarkas/ML22/deep_learning.html)).

### Szóbeágyazások, mint jellemzőtér

A deep learning modellek ún. **szóbeágyazás**okat használnak tokenek leírására. Egy szóbeágyazás egy szóhoz egy numerikus vektort rendel. Ha két vektor közel van egymáshoz (pl. euklideszi vektortávolság szerint), akkor a két szó jelentése valamilyen értelemben hasonlít egymáshoz. Precízebben, két szóvektor akkor van közel egymáshoz ha hasonló mondatkörnyezetekben fordulnak elő. Egy fajta szóbeágyazás a [word2vec](https://towardsdatascience.com/understanding-word2vec-embedding-in-practice-3e9b8985953), de a jövő héten mélyebben megismerkedünk a beágyazásokkal...

Itt most a spacy csomagot használjuk, ami tokenizál és egy saját szóbeágyazást alkalmaz.
"""

!python -m spacy download en_core_web_lg
import spacy
nlp_en = spacy.load("en_core_web_lg")

"""A szöveget először felbontjuk szavakra, a szavakat átalakítjuk vektorokká."""

text_transform = lambda x: [word.vector for word in nlp_en(x)]

text_transform("a draught beer")

"""A `pad_sequence` segítségével a vektorokat egyenlő hosszúságúvá alakítjuk, úgy, hogy a rövidebb vektorok végére 0-kból álló vektorokat írunk."""

from torch.nn.utils.rnn import pad_sequence
from torch import Tensor

pad_sequence([Tensor(text_transform("a draught beer")), Tensor(text_transform("a"))])

"""### Adatbetöltés
Betöltjük ismét az adatot.
"""

import pandas as pd

train_data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/train.tsv', sep='\t')[:2000]
test_data  = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/test.tsv', sep='\t')[:700]

train_data.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# train_data["vecs"] = train_data["text"].apply(text_transform)
# test_data["vecs"] = test_data["text"].apply(text_transform)

"""Osztálycímkéket ne felejtsük el hozzá igazítani."""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit(train_data.label)

"""A `collate_batch` függvény segítségével az adatunkat minden egyes lépésben át tudjuk alakítani."""

from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
import torch


def collate_batch(batch):
    label_list, text_list = [], []
    for row in batch:
        # Címkék átalakítása
        label_list.append(le.transform([row["label"]])[0])

        # Szöveg átalakítása
        processed_text = Tensor(row["vecs"])
        text_list.append(processed_text)

    labels_tensor = Tensor(label_list).long().to(device)

    # Szövegek egységhosszúra alakítása
    padded_vec_tensor = pad_sequence(text_list).to(device)

    return labels_tensor, padded_vec_tensor

"""A HuggingFace-es *datasets* csomag segítségével az adatot feldolgozzuk."""

!pip install datasets

from datasets import Dataset

# Dataframe-ek átalakítása Dataset-té, ami beolvasható a Data loader számára
train_dataset = Dataset.from_pandas(train_data)

test_dataset = Dataset.from_pandas(test_data)

# Data loader elkészítése, aminek megadjuk a korábban megígrt collate_batch függvényt
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,
                              collate_fn=collate_batch)

test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True,
                              collate_fn=collate_batch)

print(x := next(iter(train_dataloader)))
print(x[1].shape)

"""### CNN szerkezetének megadása

Minden feladatra saját hálózatot építhetünk az egyes neuron rétegek megadásával. Ehhez egy új osztályt kell definiálni, legalább konstruktorral és forward() metódussal.
"""

import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self, embedding_dim, n_filters, kernel_size, output_dim):
        super().__init__()

        # utána jön a konvolúciós réteg (rétegek), itt a kernel mérete az "ablakméret"
        self.conv = nn.Conv1d(in_channels = embedding_dim,
                              out_channels = n_filters,
                              kernel_size = kernel_size)

        # végül a kimeneti réteg, ami egy egyszerű lineáris réteg
        self.fc = nn.Linear(n_filters, output_dim)

    # amikor a szöveget előrefelé ("alulról felfelé" a rétegeken át) feldolgozza a háló
    def forward(self, embedded):


        # kiolvassuk a szóbeágyazási vektorokat
        # print("embedded", embedded.size())
        # ez egy 3 dimenziós tömb (tenzor):
        # embedded = [sent len, batch size, emb dim]
        embedded = embedded.permute(1, 2, 0)
        # print("embedded_perm", embedded.size())
        #embedded = [batch size, emb dim, sent len]

        # ezután a konvolúciós réteg a RelU aktivációs függvényt használja
        conved = F.relu(self.conv(embedded))
        # print("conved", conved.size())

        # ennek a tenzornak a méretei:
        # conved = [batch size, n_filters, sent len - filter_size + 1]

        # tovább tömörítjük:
        pooled = F.max_pool1d(conved, conved.shape[2]).squeeze(2)
        #print("pooled", pooled.size())
        # pooled = [batch size, n_filters]

        # a háló kimenetét a lineáris réteg számolja ki
        return self.fc(pooled)

### a háló példányosítása

embedding_dim = 300
n_filters = 40
kernel_size = 3
output_dim = len(le.classes_)

model = CNN(embedding_dim, n_filters, kernel_size, output_dim)
model = model.to(device)

# a háló rétegei:
print(model)

# összesen ennyi paramétert kell tanítanunk:
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(count_parameters(model), "tanulandó változó")

model(next(iter(train_dataloader))[1])

"""### CNN tanítása

A neurális hálók tanítása egy optimalizációs feladat megoldásával törénik. Úgy akrjuk beállítani az 1.5M változót, hogy minimalizáljuk a háló kimenete és a tényleg osztálycímke közti eltérést.
"""

### A tanítás során többször végigmegyünk a tanító adatbázison (egy kör egy epoch)

def train(model, iterator, optimizer, criterion):
    # minden epoch végén ellenőrízni fogjuk az accuracyt
    epoch_acc = 0

    model.train() # inicializálás
    for batch in iterator:
        optimizer.zero_grad()
        labels = batch[0]
        vectors = batch[1]

        # predikáljuk le a tanító példákat az aktuális paraméterekkel:
        predictions = model(vectors)

        # a háló aktuális paraméterivel ennyi a hiba a batchen:
        loss = criterion(predictions, labels)
        acc = accuracy(predictions, labels)

        # hibavisszaterjesztéssel (backpropagation) javítunk a paramétereken:
        loss.backward()
        optimizer.step()

        epoch_acc += acc.item()

    return epoch_acc / len(iterator)


def accuracy(preds, y):
    max_preds = preds.argmax(dim = 1, keepdim = True) # a 3 osztályra adott kimeneti érték közül melyik a legnagyobb
    correct = max_preds.squeeze(1).eq(y)
    return correct.sum(dtype=float) / y.shape[0]

# GPU-n akarunk tanítani:
model = CNN(embedding_dim, n_filters, kernel_size, output_dim)
model = model.to(device)

import torch.optim as optim
optimizer = optim.Adam(model.parameters()) # ADAM optimalizáló algoritmus
criterion = nn.CrossEntropyLoss() # hibafüggvény többosztályos feladatokhoz
criterion = criterion.to(device)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ### mehet a tanítás!
# 
# NUM_EPOCHS = 20
# 
# for i in range(NUM_EPOCHS):
#     print(i, ". epoch train acc:", train(model, train_dataloader, optimizer, criterion),
#             " test acc:", evaluate(model, test_dataloader))

"""### CNN kiértékelése kiértékelő halmazon"""

### ha az epoch végén egy független validációs halmazon is ki akarjuk értékelni a modellt:

def evaluate(model, iterator):
    epoch_acc = 0
    model.eval()  # inicializálás
    with torch.no_grad():
        for batch in iterator:
            # predikció
            predictions = model(batch[1])
            # kiértékelés
            acc = accuracy(predictions, batch[0])
            epoch_acc += acc.item()

    return epoch_acc / len(iterator)

evaluate(model, test_dataloader)

"""# Gyakorló fealdatok

Futtasd az órai notebook-ot, hajtsd végre az alábbi módosításokat a rendszeren!

1. Szúrj be még egy konvolúciós réteget a hálózatba!

2. Ha a konvolúció ablakméretét 5-re állítjuk, mennyi lesz a kiértékelő halmazon a pontosság?
"""# -*- coding: utf-8 -*-
"""ml_5_gyakorlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18apGyh5hduP8MKqxHEPlgTLSRpPL5aBO

# Gyakorló feladatok

Az órai adatbázison hajts végre egy kísérletet (tanítás, predikció és kiértékelés) ahol

*   a szavak szótövét vagy stemjét használjuk a szózsák modellben!
*   egy másik lineáris gépet, a dLogisztkus Regresszió osztályozó algoritmust használunk (Logistic Regression Classifier).

Írd ki, hogy mekkora szótár lesz így illetve mennyi így az accuracy!
"""

import nltk
import pandas as pd

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')
nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

#pythonban egy sorban is megoldható :)
# minden docs elemet tokenizálunk, minden tokenre stem()elünk, és a stemeket összefűzzük egy stringgé
def stem_textcol(docs):
  return [" ".join([stemmer.stem(word) for word in nltk_tokenizer.tokenize(doc)]) for doc in docs]

train_data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/train.tsv', sep='\t')
test_data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/test.tsv', sep='\t')

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

vectorizer = CountVectorizer()

filtered_train_docs = stem_textcol(train_data.text)
features = vectorizer.fit_transform(filtered_train_docs)
print("Szótárméret: ", features.shape[1] )

logreg = LogisticRegression(max_iter=5000)
model = logreg.fit(features, train_data.label)

print("Train accuracy: ", accuracy_score(y_true=train_data.label, y_pred=model.predict(features)))

filtered_test_docs = stem_textcol(test_data.text)
test_features = vectorizer.transform(filtered_test_docs)
print("Test accuracy: ", accuracy_score(y_true=test_data.label, y_pred=model.predict(test_features)))# -*- coding: utf-8 -*-
"""ml_6_embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XPmqMkECyt-b3vLr0vm8scktasgs7807

## Embeddingek/Beágyazások
Olvasd el az [elméleti bevezetőt](https://www.inf.u-szeged.hu/~rfarkas/ML22/embedding.html).

### Előtanított (pre-trained) beágyazások

Léteznek előtanított szóbeágyazások, nézzünk meg ezekből párat! Az első egy Stack Overflow fölött tanított statikus beágyazás. A másik a GLOVE ami wikipédia szövegek fölött lett tanítva.
"""

# https://github.com/vefstathiou/SO_word2vec
!wget https://zenodo.org/record/1199620/files/SO_vectors_200.bin

!wget https://nlp.stanford.edu/data/glove.6B.zip

!unzip glove.6B.zip

"""### Reprezenáció/beágyazás tanulás

Ahhoz hogy beágyazásokat gyártsunk szövegekre lesz szükségünk. Töltsünk le szöveget például Covid-19-hez köthető adatbázisból.
"""

# https://github.com/allenai/cord19
!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-04-10.tar.gz

!tar -xf cord-19_2020-04-10.tar.gz.1

"""Először is nézzünk rá az adatra amin tanítani akarunk"""

import pandas as pd

df = pd.read_csv("2020-04-10/metadata.csv")
df.head(3)

"""Tudományos cikkeket gyűjt össze az adatbázis. Ebben a táblázatban nincs benne a teljes szövege a cikkeknek azonban a custom_license fájlban megtalálhatóak.

Nekünk most elég lesznek az abstract-ok is hogy tanítsunk egy szöveg beágyazást.
"""

print(len(df))
df = df.dropna(subset=["abstract"])
print(len(df))

covid_texts = df["abstract"].tolist()

covid_texts[0]

"""Akkor tanítsunk ezek fölött egy Word2Vec modellt."""

import re  # For preprocessing
import pandas as pd  # For data handling
from time import time  # To time our operations
from collections import defaultdict  # For word frequency

import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

gensim.utils.simple_preprocess("I pull up. Then I hop out the coupe.")

covid_texts = [gensim.utils.simple_preprocess(doc) for doc in covid_texts]

"""https://radimrehurek.com/gensim/models/word2vec.html"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# w2v_model = Word2Vec(covid_texts,
#                      min_count=5,
#                      window=3,
#                      vector_size=100)

len(w2v_model.wv) # szótár mérete

w2v_model.wv["virus"]

w2v_model.wv.get_vecattr("virus", "count")

w2v_model.wv.most_similar("virus")

from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove_file = 'glove.6B.100d.txt'
tmp_file = "glove.6B.100d_w2vformat.txt"
_ = glove2word2vec(glove_file, tmp_file)
glove_w2v = KeyedVectors.load_word2vec_format(tmp_file)

stackoverflow_w2v = KeyedVectors.load_word2vec_format("SO_vectors_200.bin", binary=True)

glove_w2v.most_similar("ajax")

stackoverflow_w2v.most_similar("ajax")

glove_w2v.most_similar("pipe")

stackoverflow_w2v.most_similar("pipe")

stackoverflow_w2v.similarity("python", "java")

glove_w2v.similarity("python", "java")

stackoverflow_w2v.similarity("python", "java")

glove_w2v.most_similar("python")

"""## Szöveggenerálás

Vegyük példaként a Huggingface-ről elérhető, előtanított GPT-2 modellt, töltsük be a hozzá tartozó tokenizálót is.

Célszerű az alábbiakhoz GPU-s runtime-ra váltani
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')

tokenizer.pad_token = tokenizer.eos_token

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

model = model.to(device)

"""A tokenizáló működése:"""

input_text = "Once upon a time"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

print(input_ids, "\n")

for id in input_ids[0]:
  print(id, tokenizer.decode(id, skip_special_tokens=True))

"""A generálás folyamata, és egy lépése:"""

output = model.generate(input_ids,
                        max_length=50,
                        num_return_sequences=1,
                        pad_token_id=tokenizer.pad_token_id)


generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

import torch.nn.functional as F

with torch.no_grad():
    outputs = model(input_ids)

# logit-ok az utolsó kimeneti tokenre
last_token_logits = outputs.logits[0, -1, :]

# alkalmazzunk softmax-et, hogy valószínűségi értékeket kapjunk
probs = F.softmax(last_token_logits, dim=-1)

# az 5 legmagasabb valószínűségi értékkel rendelkező token
top_k_probs, top_k_indices = torch.topk(probs, 5)

top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices]

for i, (token, prob) in enumerate(zip(top_k_tokens, top_k_probs)):
    print(f"Top {i+1} token: '{token}', probability:", round(prob.item(), 3), "-->", input_text + f"{token}")

"""### Fine-tuning

Szeretnénk a fenti modellt finomhangolni arra, hogy "rossz online értékeléseket" írjon. Ehhez egy online szöveges értékeléseket tartalmazó adatbázisból használjuk az 1 és 2 csillagos review-kat.
"""

!pip install datasets

import pandas as pd
from datasets import Dataset

df = pd.read_parquet("hf://datasets/Yelp/yelp_review_full/yelp_review_full/train-00000-of-00001.parquet")
df = df[df.label < 2]
df = df[["text"]].iloc[:10_000]

dataset = Dataset.from_pandas(df)
dataset = dataset.remove_columns("__index_level_0__")

def tokenize_function(examples):
    encoding = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)
    encoding['labels'] = encoding['input_ids'].copy()
    return encoding

tokenized_datasets = dataset.map(tokenize_function, batched=True)

tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

"""Az alábbi lépés, a finomhangolás folyamata GPU-n körülbelül 5 percet vesz igénybe."""

import os
from transformers import Trainer, TrainingArguments

os.environ["WANDB_DISABLED"] = "true"

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=16,
    logging_dir='./logs',
    logging_steps=50,
    report_to=[],
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
)

trainer.train()

trainer.save_model('./results')

"""Nézzük meg, hogy az eredeti és a finomhangolt modell mit generál, ha "The restaurant" szavakkal promptoljuk be őket."""

input_text = "The restaurant"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

model = AutoModelForCausalLM.from_pretrained('gpt2')
model = model.to(device)

pre_trained_output = model.generate(input_ids, max_length=25, pad_token_id=tokenizer.pad_token_id)
pre_trained_text = tokenizer.decode(pre_trained_output[0], skip_special_tokens=True)
print("Pre-trained model output:")
print(pre_trained_text)



fine_tuned_model = AutoModelForCausalLM.from_pretrained('./results').to(device)

fine_tuned_output = fine_tuned_model.generate(input_ids, max_length=25, pad_token_id=tokenizer.pad_token_id)
fine_tuned_text = tokenizer.decode(fine_tuned_output[0], skip_special_tokens=True)
print("\nFine-tuned model output:")
print(fine_tuned_text)

# -*- coding: utf-8 -*-
"""ml_7_gyakorlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v95Zih_fOC6My8Cg7mhWPYWTJVJaAUH1

**1)** A megismert 100 kutya/macska adatbázison futtasd le az alábbi kísérletet.
*    szürkeárnyalatos képeken
*    64 x 64-re átméretezve
*    train/test 80-20% vágás
*    sztochasztikus gradient descent tanulóval
*    jellemzőkészlet: pixelek

Kérdés, hogy mennyire függ az elért pontosság a train test split véletlenszerűségétől! Ehhez százszor futtasd le, hogy
a 100 képből véletlenszerűen választasz 80 train és 20 test esetet tanítasz a trainen, accuracyt mérsz a teszten.
"""

# képadatbázis letöltése

import os
import urllib.request
import zipfile

url = 'https://github.com/rfarkas/student_data/raw/main/images/100imgs.zip'
urllib.request.urlretrieve(url,'t.zip')
zipfile.ZipFile('t.zip').extractall('tmp_imgs')

# jellemzőkinyerés

import cv2 as cv #OpenCV

features = []
labels = []

for f in os.listdir('tmp_imgs'):
  image = cv.imread('tmp_imgs/'+f)
  label = f.split(os.path.sep)[-1].split(".")[0]
  small_image = cv.resize(image,(64,64))
  small_gray_image = cv.cvtColor(small_image,cv.COLOR_BGR2GRAY)
  pixel = small_gray_image.flatten()
  labels.append(label)
  features.append(pixel)

# gépi tanulási kísérlet

from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split

accuracy = []
cl = SGDClassifier()
for i in range(100):
  X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2) # véletlenszerű train/test szétvágása a 100 képnek
  model = cl.fit(X_train, y_train) # tanítás train halmazon
  accuracy.append(model.score(X_test, y_test)) # kiértékelés a test halmazon

# eredmények vizualizációja

import pandas as pd
pd.Series(accuracy).hist()

"""Ilyen kicsi adatbázison, az eredmények nagyban függenek a train/test vágástól!!! Célszerű legalább párezres adatbázison kiértékelni, hogy robosztusabbak legyenek az eredmények.

**2)** Milyen pontossággal tudjuk egy arcképről azonosítani az érzelmeket?

Hajts végre egy tanító-kiértékelő adatbázis bontással predikciós kísérletet az alábbi adatbázison!
"""

import pandas as pd
data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/images/fer2013_1K.csv')

data

print(data.columns)
len(data)

#48x48 szürke képek egy stringbe vannak kódolva. A 11. kép kiolvasása:
import numpy as np
image = np.reshape(np.asarray(data.pixels[11].split(' '), np.uint8), (48,48))
plt.imshow(image, cmap="gray")

def str2img(img):
  return np.reshape(np.asarray(img.split(' '), np.uint8), (48,48))

!wget https://raw.githubusercontent.com/christiansafka/img2vec/master/img2vec_pytorch/img_to_vec.py

from img_to_vec import Img2Vec
img2vec = Img2Vec()

from PIL import Image
img2vec.get_vec(Image.fromarray(image).convert('RGB')) # img2vec csak RGB képeket tud kezelni, a szürkeárnyalatos képeinket RGB-be kell konvertálni!

imgvecs = []
for i in data.pixels:
  imgvec = img2vec.get_vec(Image.fromarray(str2img(i)).convert('RGB'))
  imgvecs.append(imgvec)

# train/test vágás
from sklearn.model_selection import train_test_split
labels = data.emotion.tolist()
train_features, test_features, train_labels, test_label = train_test_split(imgvecs,labels, test_size=0.2)

# KNN osztályozó tanítása
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(train_features, train_labels)

# Kiértékelés a teszt halmazon
from sklearn.metrics import classification_report

pred = model.predict(test_features)
print(classification_report(y_pred=pred, y_true=test_label))

# az eredményeket mindig hasonlítsuk egy egyszerű döntési szabály eredményeihez!
from sklearn.dummy import DummyClassifier
dummy = DummyClassifier(strategy='most_frequent',random_state=0)
dummy.fit(train_features, train_labels)

pred_dummy = dummy.predict(test_features)
print(classification_report(y_pred=pred_dummy, y_true=test_label))# -*- coding: utf-8 -*-
"""ml_8_gyakorlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lhmlblSuNfGO6UDIl57cxD-teJsLzOrD

# Gyakorló feladat
Az https://archive.ics.uci.edu/ml/machine-learning-databases/00432/Data/News_Final.csv adatbázis tartalmazza (többek közt), hogy egy megjelent újságcikket, a rákövetkező héten hányan likeoltak a Facebookon. Hajts végre egy gépi tanulási kísérletet, hogy megtudjuk, hogy a `headline` szövege és a `source` alapján mennyire jól lehet megjósolni a `Facebook` likeok számát!
"""

# adat beolvasása
import pandas as pd
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00432/Data/News_Final.csv')
df

"""
Csak azokat a cikkeket használd, ahol pozitív a `Facebook` likeok száma! Tanító adatbázisnak használd a 2016 máriusáig megjelent (`PublishDate`) cikkeket és értékeld ki a 2016. ápr 1. utáni cikkeken!"""

# időpontként és ne string-ként kezeljük a PublishDate-et
df.PublishDate = pd.to_datetime(df.PublishDate)

# nem pozitív Facebook like értékkel bíró sorokat eldobjuk
df = df[df.Facebook>0]

# Tanító és kiértékelő adatbázisra vágunk
traindf = df[df.PublishDate <= '2016-04-01']
testdf = df[df.PublishDate > '2016-04-01']
print("TanítóDB méretei:", traindf.shape)
print("KiértékelőDB méretei:", testdf.shape)

# Jellemzőkinyerés a headline és source szöveges mezőkből
from sklearn.feature_extraction.text import CountVectorizer

bow_extractor = CountVectorizer()
train_features = bow_extractor.fit_transform(traindf.Headline + traindf.Source) # string oszlopok összeadása a sorok szövegeinek összefűzését jelenti
# ez elszáll a NaN-ok miatt...

# dobjuk a NaNos sorokat
traindf = traindf.dropna()
testdf = testdf.dropna()
print("TanítóDB méretei:", traindf.shape)
print("KiértékelőDB méretei:", testdf.shape)
# nem sokat veszítettünk

# így már lefut
train_features = bow_extractor.fit_transform(traindf.Headline + traindf.Source)
print(train_features.shape)
# 55ezer szavunk=jellemzőnk van

# Regressziós modell tanulása
from sklearn.linear_model import LinearRegression

reg = LinearRegression(normalize=True)
reg.fit(train_features, traindf.Facebook) # Facebook oszlop a célváltozó
# ez így nagyon lassú... inkább állítsuk le a futást.

# Csökkentsük a tanító adatbázis méretét, hogy lássunk valami eredményt!
# vagy a sorok számát csökkentjük (traindf_small = traindf[:2000]) vagy a jellemzők számát
# nézzük a jellemzőszámot 55ezerről hogy lehet csökkenteni:
bow_extractor = CountVectorizer(min_df=10) # min_df=N esetén csak azok a szavak kerülnek be amelyek legalább N dokumentumban előfordulnak. A ritkák úgyis csak zajt hoznak be...
train_features = bow_extractor.fit_transform(traindf.Headline + traindf.Source)
print(train_features.shape) # így már csak 7254 jellemző maradt

# és gyorsan lefut a tanulás a teljes halmazon is
reg = LinearRegression()
reg.fit(train_features, traindf.Facebook)

# predikció és kiértékelés a kiértékelő adatbázisra
test_features = bow_extractor.transform(testdf.Headline + testdf.Source) # a transform a fit-nél kiválasztott szavakat használja csak
prediction = reg.predict(test_features)
from sklearn.metrics import mean_squared_error
mean_squared_error(prediction, testdf.Facebook)

### Mindig hasonlítsuk az eredményt baselinehoz!
from sklearn.dummy import DummyRegressor
dummy = DummyRegressor(strategy='mean') # tanító adatbázis címkéinek átlaga lesz mindig a predikció
dummy.fit(train_features, traindf.Facebook)
mean_squared_error(dummy.predict(test_features), testdf.Facebook)

"""Sokkal rosszabb a tanuló, mint a baseline :(

Mivel szövegekről van szó, érdemes lehet kipróbálni a regressziós döntési fát is (talán ennyi jellemzőre még lefut).
"""

from sklearn.tree import DecisionTreeRegressor
reg = DecisionTreeRegressor(min_samples_leaf=5000) # döntési fa regresszióra

reg.fit(train_features, traindf.Facebook)
prediction = reg.predict(test_features)
mean_squared_error(prediction, testdf.Facebook)
# veri a baselinet ...# -*- coding: utf-8 -*-
"""ml_9_gyakorlo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RhVk2_LlRIiISJV5BMWGydI1LMlepeun
"""

import pandas as pd
df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", sep=";")
df

df.quality.hist(bins=7)

labels = df.quality # regressziós célváltozó
features = df.iloc[:,:-1] # jellemzőtér

from sklearn.model_selection import train_test_split
t_features, test_features, t_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42) # kiértékelő adatbzásis levágása
train_features, valid_features, train_labels, valid_labels = train_test_split(t_features, t_labels, test_size=0.2, random_state=42) # tanító és validációs halmazok leválasztása

# MSE-t használunk kiértékelési metrikaként
from sklearn.metrics import mean_squared_error

### döntési fa meta-paraméter hangolással
from sklearn import tree
for d in range(1,20):
  dt = tree.DecisionTreeRegressor(max_depth=d) # döntési fa mélysége
  dt.fit(train_features, train_labels)  # tanítunk a tanító adatbázison
  prediction = dt.predict(valid_features)
  print(d,":",mean_squared_error(prediction, valid_labels))

"""5 mélységgel legkisebb a hiba"""

# az SGD nagyon érzékeny a jellemzők értékkészletének különbségeire (ebben a feladatban a jellemzők különböző skálán mozognak)
# skálázzunk minden jellemzőt ugyanarra az intervallumra
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_train = scaler.fit_transform(train_features)
scaled_valid = scaler.transform(valid_features)

### Lineáris gép meta-paraméter hangolással
from sklearn.linear_model import SGDRegressor
for d in range(-15,3):
  lin = SGDRegressor(alpha=10**d) # regularizációs együttható
  lin.fit(scaled_train, train_labels)
  prediction = lin.predict(scaled_valid)
  print(d,":",mean_squared_error(prediction, valid_labels))

"""Nem tud jobban, mint a döntési fa..."""

### kNN meta-paraméter hangolással
from sklearn.neighbors import KNeighborsRegressor
for k in range(1,50,2): # páratlan k értékek
  knn = KNeighborsRegressor(n_neighbors=k) # kNN különböző k értékekkel
  knn.fit(scaled_train, train_labels)
  prediction = knn.predict(scaled_valid)
  print(k,":",mean_squared_error(prediction, valid_labels))

"""k=15-el a kNN a legjobb (skálázás nélkül is próbáld ki :D )

## Végső kiértékelés a kiértékelő halmazon
"""

### Teljes címkézett halmazt (train+valid) használhatjuk a végső kiértékeléshez
scaler = StandardScaler()
scaled_train = scaler.fit_transform(t_features)
scaled_test  = scaler.transform(test_features)

# k=15 kNN a legjobb modellünk:
knn = KNeighborsRegressor(n_neighbors=15)
knn.fit(scaled_train, t_labels)
prediction = knn.predict(scaled_test)
mean_squared_error(prediction, test_labels)

"""Ne felejtsük el baseline-hoz hasonlítani!"""

from sklearn.dummy import DummyRegressor
dummy = DummyRegressor(strategy='mean') # tanító adatbázis címkéinek átlaga lesz mindig a predikció
dummy.fit(scaled_train, t_labels)
mean_squared_error(dummy.predict(scaled_test), test_labels)

"""A regresszorunk átlagos hibája jóval kisebb, mint a baseline-é!"""
