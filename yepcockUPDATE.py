# -*- coding: utf-8 -*-
"""yepcock.ipynb másolata

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F3oE0_LeuGlVvEV52Tdv5PICCjzneXZq
"""

import pandas as pd
import numpy as np
from sklearn.dummy import DummyClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn import tree, preprocessing

"""# 1. feladat

Az https://github.com/kiscsonti/student_data/raw/main/mushroom_dataset.zip fájl egy adatbázit tartalmaz gombákról. Töltsd le és írj egy egyszerű döntési szabályt (egy jellemzőből), ami jobban dönti el, hogy ehető-e a gomba (`class=e`) mint a leggyakoribb osztály baseline. Kiértékelési metrikaként a mérgező osztály (`class=p`) F1 Score-ját használd!

## A jellemzőkről leírás:

* class: edible=e, poisonous=p
* cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s
* gill-attachment: attached=a,descending=d,free=f,notched=n
* stalk-shape: enlarging=e,tapering=t
* stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?
* veil-type: partial=p,universal=u
* veil-color: brown=n,orange=o,white=w,yellow=y
* ring-number: none=n,one=o,two=t
"""

dataset = pd.read_csv("https://raw.githubusercontent.com/kiscsonti/student_data/main/mushroom.csv")
dataset.head()

# egy diszkrét jellemző alapján
pd.crosstab(dataset["class"], dataset["cap-surface"])

classlabel = dataset["class"]
features = dataset.iloc[:,1:]

dummy_clf = DummyClassifier(strategy="most_frequent") # tanító adatbázis leggyakoribb osztálya lesz mindig a predikció
dummy_clf.fit(features, classlabel) # ugyanazon a tanító adatbázison "tanítjuk"
baseline_prediction = dummy_clf.predict(features) # predikció a kiértékelő adatbázison
print(f1_score(classlabel,baseline_prediction,pos_label='p'))
#print(accuracy_score(baseline_prediction,classlabel))
print(classification_report(classlabel, baseline_prediction))

pred = pd.Series(['e'] * len(dataset)) # egy konstans 'p'-kat tartalamazó Series
pred[dataset["cap-surface"] == 'y'] = 'p'

print(f1_score(classlabel,pred,pos_label='p'))
#print(accuracy_score(pred,classlabel))
print(classification_report(classlabel, pred))

"""# 2. feladat

Hajts végre gépi tanulási kísérletet arra nézve, hogy egy gomba mérgező/ehető mennyire jól állapítható meg a jellemzők alapján! Használd az adat véletlenszerű 20%-át kiértékelő adatbázisnak.
"""

import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.metrics import f1_score, classification_report


# OneHotEncoder alkalmazása
ohe = preprocessing.OneHotEncoder()  # sparse mátrix formátum
ohe_features = ohe.fit_transform(features)

# Képzés és tesztelés
features_train, features_test, classlabel_train, classlabel_test = train_test_split(ohe_features, classlabel, test_size=0.2, random_state=42)

# Modell betanítása
dt = tree.DecisionTreeClassifier()
dt.fit(features_train, classlabel_train)

# Előrejelzés
results = dt.predict(features_test)

# Részletes osztályozási riport
print(classification_report(classlabel_test, results))

# F1-score kiértékelés
print(f"F1-Score: {f1_score(classlabel_test, results, pos_label='p'):.4f}")



"""# 3. feladat
Próbálj ki egy másik gépi tanuló algoritmust is a 2. feladatra. Mindkettőnek hangold be a meta-paraméterit. (bónus feladat: a meta-paraméter értékekre a túltanási ábra kirajzolása)

A végén szövegként írd le, hogy a két algoritmus közül melyik a jobb!

"""

from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier() # n_neighbors a k érték (szomszédok száma)

from sklearn.metrics import accuracy_score
model.fit(features_train,classlabel_train)
prediction = model.predict(features_test)
f1_score(classlabel_test,prediction ,pos_label='p')

"""DT-hangolás"""

### különböző fa mélységi értékekkel tanítjuk a fát
dt_valid_f1=[]
dt_train_f1=[]
for d in range(1,40):
  dt = tree.DecisionTreeClassifier(max_depth=d) # döntési fa mélysége
  dt.fit(features_train,classlabel_train)  # tanítunk a tanító adatbázison
  valid_prediction = dt.predict(features_test)
  dt_valid_f1.append(f1_score(classlabel_test, valid_prediction, pos_label="p")) # kiértékelés a validációs halmazon
  train_prediction = dt.predict(features_train)
  dt_train_f1.append(f1_score(classlabel_train, train_prediction, pos_label="p")) # a tanító adatbázison is kiértékeljük a túltanulási vizsgálatokhoz

### jelenítsük meg az eredményeket
import matplotlib.pyplot as plt
plt.figure(figsize=(10,10))
plt.plot(dt_valid_f1, c="green")
plt.plot(dt_train_f1, c="red")

"""KNN-hangolás"""

### különböző fa mélységi értékekkel tanítjuk a fát
knn_valid_f1=[]
knn_train_f1=[]
for k in range(1,20,2): # páratlan k értékek
  knn = KNeighborsClassifier(n_neighbors=k) # kNN különböző k értékekkel
  knn.fit(features_train, classlabel_train)
  valid_prediction = knn.predict(features_test)
  knn_valid_f1.append(f1_score(classlabel_test, valid_prediction, pos_label="p"))
  train_prediction = knn.predict(features_train)
  knn_train_f1.append(f1_score(classlabel_train, train_prediction, pos_label="p"))

plt.figure(figsize=(10,10))
plt.plot(knn_valid_f1, c="green")
plt.plot(knn_train_f1, c="red")

# KNN legjobb predikció
model = KNeighborsClassifier(n_neighbors=4) # n_neighbors a k érték (szomszédok száma)
model.fit(features_train,classlabel_train)
prediction = model.predict(features_test)
knnf1 = f1_score(classlabel_test,prediction ,pos_label='p')

# DT legjobb predikció
dt = tree.DecisionTreeClassifier(max_depth=4) # döntési fa mélysége
dt.fit(features_train,classlabel_train)  # tanítunk a tanító adatbázison
valid_prediction = dt.predict(features_test)
dtf1 = f1_score(classlabel_test,valid_prediction,pos_label='p')

print("dtf1:")
print(dtf1)
print("knnf1:")
print(knnf1)

if dtf1>knnf1:
  print("DT jobban predikált: ")

else:
  print("KNN jobban predikált: ")

"""A KNN jobban predikált azonos feltételek mellett."""
