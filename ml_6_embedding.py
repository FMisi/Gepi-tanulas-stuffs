# -*- coding: utf-8 -*-
"""ml_6_embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XPmqMkECyt-b3vLr0vm8scktasgs7807

## Embeddingek/Beágyazások
Olvasd el az [elméleti bevezetőt](https://www.inf.u-szeged.hu/~rfarkas/ML22/embedding.html).

### Előtanított (pre-trained) beágyazások

Léteznek előtanított szóbeágyazások, nézzünk meg ezekből párat! Az első egy Stack Overflow fölött tanított statikus beágyazás. A másik a GLOVE ami wikipédia szövegek fölött lett tanítva.
"""

# https://github.com/vefstathiou/SO_word2vec
!wget https://zenodo.org/record/1199620/files/SO_vectors_200.bin

!wget https://nlp.stanford.edu/data/glove.6B.zip

!unzip glove.6B.zip

"""### Reprezenáció/beágyazás tanulás

Ahhoz hogy beágyazásokat gyártsunk szövegekre lesz szükségünk. Töltsünk le szöveget például Covid-19-hez köthető adatbázisból.
"""

# https://github.com/allenai/cord19
!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-04-10.tar.gz

!tar -xf cord-19_2020-04-10.tar.gz.1

"""Először is nézzünk rá az adatra amin tanítani akarunk"""

import pandas as pd

df = pd.read_csv("2020-04-10/metadata.csv")
df.head(3)

"""Tudományos cikkeket gyűjt össze az adatbázis. Ebben a táblázatban nincs benne a teljes szövege a cikkeknek azonban a custom_license fájlban megtalálhatóak.

Nekünk most elég lesznek az abstract-ok is hogy tanítsunk egy szöveg beágyazást.
"""

print(len(df))
df = df.dropna(subset=["abstract"])
print(len(df))

covid_texts = df["abstract"].tolist()

covid_texts[0]

"""Akkor tanítsunk ezek fölött egy Word2Vec modellt."""

import re  # For preprocessing
import pandas as pd  # For data handling
from time import time  # To time our operations
from collections import defaultdict  # For word frequency

import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

gensim.utils.simple_preprocess("I pull up. Then I hop out the coupe.")

covid_texts = [gensim.utils.simple_preprocess(doc) for doc in covid_texts]

"""https://radimrehurek.com/gensim/models/word2vec.html"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# w2v_model = Word2Vec(covid_texts,
#                      min_count=5,
#                      window=3,
#                      vector_size=100)

len(w2v_model.wv) # szótár mérete

w2v_model.wv["virus"]

w2v_model.wv.get_vecattr("virus", "count")

w2v_model.wv.most_similar("virus")

from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove_file = 'glove.6B.100d.txt'
tmp_file = "glove.6B.100d_w2vformat.txt"
_ = glove2word2vec(glove_file, tmp_file)
glove_w2v = KeyedVectors.load_word2vec_format(tmp_file)

stackoverflow_w2v = KeyedVectors.load_word2vec_format("SO_vectors_200.bin", binary=True)

glove_w2v.most_similar("ajax")

stackoverflow_w2v.most_similar("ajax")

glove_w2v.most_similar("pipe")

stackoverflow_w2v.most_similar("pipe")

stackoverflow_w2v.similarity("python", "java")

glove_w2v.similarity("python", "java")

stackoverflow_w2v.similarity("python", "java")

glove_w2v.most_similar("python")

"""## Szöveggenerálás

Vegyük példaként a Huggingface-ről elérhető, előtanított GPT-2 modellt, töltsük be a hozzá tartozó tokenizálót is.

Célszerű az alábbiakhoz GPU-s runtime-ra váltani
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')

tokenizer.pad_token = tokenizer.eos_token

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

model = model.to(device)

"""A tokenizáló működése:"""

input_text = "Once upon a time"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

print(input_ids, "\n")

for id in input_ids[0]:
  print(id, tokenizer.decode(id, skip_special_tokens=True))

"""A generálás folyamata, és egy lépése:"""

output = model.generate(input_ids,
                        max_length=50,
                        num_return_sequences=1,
                        pad_token_id=tokenizer.pad_token_id)


generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

import torch.nn.functional as F

with torch.no_grad():
    outputs = model(input_ids)

# logit-ok az utolsó kimeneti tokenre
last_token_logits = outputs.logits[0, -1, :]

# alkalmazzunk softmax-et, hogy valószínűségi értékeket kapjunk
probs = F.softmax(last_token_logits, dim=-1)

# az 5 legmagasabb valószínűségi értékkel rendelkező token
top_k_probs, top_k_indices = torch.topk(probs, 5)

top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices]

for i, (token, prob) in enumerate(zip(top_k_tokens, top_k_probs)):
    print(f"Top {i+1} token: '{token}', probability:", round(prob.item(), 3), "-->", input_text + f"{token}")

"""### Fine-tuning

Szeretnénk a fenti modellt finomhangolni arra, hogy "rossz online értékeléseket" írjon. Ehhez egy online szöveges értékeléseket tartalmazó adatbázisból használjuk az 1 és 2 csillagos review-kat.
"""

!pip install datasets

import pandas as pd
from datasets import Dataset

df = pd.read_parquet("hf://datasets/Yelp/yelp_review_full/yelp_review_full/train-00000-of-00001.parquet")
df = df[df.label < 2]
df = df[["text"]].iloc[:10_000]

dataset = Dataset.from_pandas(df)
dataset = dataset.remove_columns("__index_level_0__")

def tokenize_function(examples):
    encoding = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)
    encoding['labels'] = encoding['input_ids'].copy()
    return encoding

tokenized_datasets = dataset.map(tokenize_function, batched=True)

tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

"""Az alábbi lépés, a finomhangolás folyamata GPU-n körülbelül 5 percet vesz igénybe."""

import os
from transformers import Trainer, TrainingArguments

os.environ["WANDB_DISABLED"] = "true"

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=16,
    logging_dir='./logs',
    logging_steps=50,
    report_to=[],
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
)

trainer.train()

trainer.save_model('./results')

"""Nézzük meg, hogy az eredeti és a finomhangolt modell mit generál, ha "The restaurant" szavakkal promptoljuk be őket."""

input_text = "The restaurant"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

model = AutoModelForCausalLM.from_pretrained('gpt2')
model = model.to(device)

pre_trained_output = model.generate(input_ids, max_length=25, pad_token_id=tokenizer.pad_token_id)
pre_trained_text = tokenizer.decode(pre_trained_output[0], skip_special_tokens=True)
print("Pre-trained model output:")
print(pre_trained_text)



fine_tuned_model = AutoModelForCausalLM.from_pretrained('./results').to(device)

fine_tuned_output = fine_tuned_model.generate(input_ids, max_length=25, pad_token_id=tokenizer.pad_token_id)
fine_tuned_text = tokenizer.decode(fine_tuned_output[0], skip_special_tokens=True)
print("\nFine-tuned model output:")
print(fine_tuned_text)

