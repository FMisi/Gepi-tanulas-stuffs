# -*- coding: utf-8 -*-
"""GN_ZH.ipynb másolata

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HZE2x3p89HsQyIes55n4feRbzu3im8kW

<div class="markdown-google-sans">

# Zárthelyi dolgozat
</div>

- Másold le a dokumentumot a megszokott módon és nevezd át, úgy, hogy az tartalmazza a nevedet és a Neptun-kódodat (Vezetknév_Keresztnév-NEPTUNKÓD)!
- Miután átnevezted, oszd meg a dokumentumot velünk szerkesztési joggal! (az email-címeinket a táblán találod)


- Kérünk benneteket, hogy ahol csak lehetséges, legjobb tudomásotok szerint kommentezzétek fel a kódot, illetve válaszoljatok szövegesen is.
- A hangsúly azon van, ahogyan előállítjátok a kért adatokat, nem azok pontosságán, így ha valahol hibás predikációt ad a modell nem gond, (de lehetőség szerint ismerjétek fel, és írjátok oda, hogy mi lenne az elvárt viselkedés szerintetek).
- <u>Ezen felül, ha előre láthatóan ki fogtok futni az időből, akkor lehetőség szerint írjátok le szövegesen, mit csináltatok volna a maradék feladatokkal!</u>


- A zárthelyi megírásához használhattok bármilyen segédanyagot, kivéve egymást. **Se online, se offline!**


**Sok Sikert Kívánunk!**

---

# 1. feladat

Az https://github.com/rfarkas/student_data/raw/main/images/500faces.zip fájl arcképeket tartalmaz. Hajts végre regressziós gépi tanulási kísérletet arra nézve, hogy az életkor mennyire jól állapítható meg képek alapján! A fájlok nevének első `_` előtti része tartalmazza az igazi életkort. Használd a képek véletlenszerű 20%-át kiértékelő adatbázisnak.

Érdemes a képeket kisebbre átméretezni, esetleg szürkeárnyalossá konvertálni, hogy gyorsabban lehessen kísérletezni.
"""

import os
import urllib.request
import zipfile
import numpy as np
import cv2 as cv
from sklearn.linear_model import SGDRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Download and extract the dataset
url = 'https://github.com/rfarkas/student_data/raw/main/images/500faces.zip'
urllib.request.urlretrieve(url, 't.zip')
zipfile.ZipFile('t.zip').extractall('tm_imgs')

# Initialize features and labels
features = []
labels = []

# Read images and extract features
for f in os.listdir('tm_imgs'):
    image = cv.imread(os.path.join('tm_imgs', f))  # Load image
    label = f.split("_")[0]  # Extract age from the filename
    small_image = cv.resize(image, (16, 16))  # Resize image to 16x16
    small_gray_image = cv.cvtColor(small_image, cv.COLOR_BGR2GRAY)  # Convert to grayscale
    pixel = small_gray_image.flatten()  # Flatten into a 1D array
    labels.append(float(label))  # Convert label to float
    features.append(pixel)

# Convert to NumPy arrays
features = np.array(features)
labels = np.array(labels)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize the model
cl = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)

# Train the model and evaluate
cl.fit(X_train, y_train)
y_pred = cl.predict(X_test)

# Metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

### regressziós gépi tanulási kísérlet
from sklearn.linear_model import LinearRegression

reg = LinearRegression() # lineáris gép regresszióra
reg.fit(X_train, y_train)
prediction = reg.predict(X_test)

from sklearn.metrics import mean_squared_error # MSE (RMSE végső gyökvonás nélkül)
mean_squared_error(prediction, y_test)

from sklearn.dummy import DummyRegressor
dummy = DummyRegressor(strategy='mean')
dummy.fit(X_train, y_train)
mean_squared_error(dummy.predict(X_test), y_test)

#baseline megoldás jobb mint a linear regresszó (415.4381737370029 < 589.9170657476824)

"""# 2. feladat
Próbálj ki egy másik regressziós algoritmust is az 1. feladatra. Mindkét módszernek hangold be a meta-paraméterit. (bónus feladat: a meta-paraméter értékekre a túltanási ábra kirajzolása)

A végén szövegként írd le, hogy a két algoritmus közül melyik a jobb!

"""

#X_train, X_test, y_train, y_test
from sklearn import tree
dt = tree.DecisionTreeClassifier()
dt.fit(X_train, y_train)

!pip install scikit-learn==1.0.2
!pip install eli5
import eli5

from sklearn.metrics import f1_score
valid_f1=[]
train_f1=[]
for d in range(1,40):
  dt = tree.DecisionTreeClassifier(max_depth=d)
  dt.fit(X_train, y_train)
  valid_prediction = dt.predict(X_test)
  valid_f1.append(f1_score(y_test, valid_prediction, average='micro'))
  train_prediction = dt.predict(X_train)
  train_f1.append(f1_score(y_train, train_prediction, average='micro'))

import matplotlib.pyplot as plt
plt.figure(figsize=(10,10))
plt.plot(valid_f1, c="green")
plt.plot(train_f1, c="red")

from sklearn.metrics import classification_report

dt = tree.DecisionTreeClassifier(max_depth=32)
dt.fit(X_train, y_train)
valid_prediction = dt.predict(X_test)
print(classification_report(y_test, valid_prediction))
print("Legjobb döntési fa a validációs halmazon:", f1_score(y_test, valid_prediction, average='micro'))##itt félbe kellett hagynom

"""# 3. feladat
Klaszterezd a képeket minden klaszterből jeleníts meg 3-3db képet, hogy kapjunk egy benyomást, hogy mik kerültek egy klaszterbe.
"""

# Import necessary libraries for clustering and visualization
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np

# Define the number of clusters (you can adjust this)
num_clusters = 5  # You can change this depending on your preference

# Initialize and fit the KMeans model
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(features)

# Get the labels (cluster assignments) for each image
cluster_labels = kmeans.labels_

# Plotting function to show 3 images from each cluster
def plot_images_from_cluster(cluster_labels, features, num_clusters, images_per_cluster=3):
    # Calculate the number of subplots needed
    total_images = num_clusters * images_per_cluster
    rows = (total_images + images_per_cluster - 1) // images_per_cluster  # Round up to the next full row

    plt.figure(figsize=(15, rows * 5))  # Dynamically adjust the figure size based on the number of images
    cluster_idx = 0

    # Loop through each cluster
    for cluster in range(num_clusters):
        cluster_images = np.where(cluster_labels == cluster)[0]  # Get the image indices for the current cluster

        # Loop through and display a few images from each cluster
        for i in range(min(images_per_cluster, len(cluster_images))):
            img_index = cluster_images[i]
            image = features[img_index].reshape(16, 16)  # Reshape the image to 16x16 for display

            # Create a subplot for each image
            plt.subplot(rows, images_per_cluster, cluster_idx + 1)
            plt.imshow(image, cmap='gray')
            plt.axis('off')  # Hide axis for better visualization
            plt.title(f'Cluster {cluster + 1}')
            cluster_idx += 1

            if cluster_idx >= total_images:
                break  # Stop plotting if we've reached the desired number of images

    plt.show()

# Display images from each cluster
plot_images_from_cluster(cluster_labels, features, num_clusters=5, images_per_cluster=3)